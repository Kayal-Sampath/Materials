{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tamil_hate.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i52TZgjnPdY"
      },
      "outputs": [],
      "source": [
        "# importing transformers and datasets (from HuggingFace)\n",
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tamil hate speech classification - dataset"
      ],
      "metadata": {
        "id": "g1kFaCbbqyhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import datasets from local colab drive\n",
        "#import transformers\n",
        "#from datasets import load_dataset\n",
        "#dataset=load_dataset('csv',data_files={'train':'sample_data/train.csv','test':'sample_data/eval.csv'})"
      ],
      "metadata": {
        "id": "LYSj-uPinf83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dataset from the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "EXBDL906pMav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset=load_dataset('csv',data_files={'train':'drive/MyDrive/train.csv','eval':'drive/MyDrive/eval.csv'})"
      ],
      "metadata": {
        "id": "IeQuat5kyf3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "v3i0KKJzMav0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XgTpm9ZxoN9"
      },
      "outputs": [],
      "source": [
        "dataset['train'][201]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['eval'][101]"
      ],
      "metadata": {
        "id": "oy_OqFF7ziWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'].column_names"
      ],
      "metadata": {
        "id": "QqT-H7KVaUC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'].num_rows"
      ],
      "metadata": {
        "id": "jQbKTvGfzy_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-Training using BERT. The contextual representation of the word(sub-tokens) \n",
        "# from the pre-trained BERT model\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'],padding='max_length',truncation=True,max_length=100)\n",
        "tokenize_function(dataset[\"train\"][:2])"
      ],
      "metadata": {
        "id": "CDO9pZiN2DC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets=dataset.map(tokenize_function,batched=True)\n",
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "id": "3LROoyBb2jQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=tokenized_datasets['train'].shuffle(seed=42).select(range(500))\n",
        "eval_dataset=tokenized_datasets['eval'].shuffle(seed=42).select(range(500))\n",
        "\n",
        "#train_dataset.column_names\n",
        "print(train_dataset['text'][201])"
      ],
      "metadata": {
        "id": "K__FzO1V2t_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode_sentence=tokenizer.decode(tokenized_datasets['train']['input_ids'][601])\n",
        "print(decode_sentence)"
      ],
      "metadata": {
        "id": "YMfldvcW2-RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-Training using RoBERTa. The contextual representation of the word(sub-tokens) \n",
        "# from the pre-trained BERT model\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'],padding='max_length',truncation=True,max_length=50)\n",
        "tokenize_function(dataset[\"train\"][:2])"
      ],
      "metadata": {
        "id": "02ZV9cb5BP8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input ids are the only required parameters to be passed to the model as input. \n",
        "\n",
        "tokenized_datasets=dataset.map(tokenize_function,batched=True)\n",
        "print(tokenized_datasets)\n",
        "print(tokenized_datasets['train']['text'][201])"
      ],
      "metadata": {
        "id": "KoJU0Ko8z4FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##The tokens are either words or subwords.These tokens can then be converted into IDs which are understandable by the model.\n",
        "train_dataset=tokenized_datasets['train'].shuffle(seed=42).select(range(500))\n",
        "eval_dataset=tokenized_datasets['eval'].shuffle(seed=42).select(range(500))\n",
        "\n",
        "#train_dataset.column_names\n",
        "print(train_dataset['text'][201])"
      ],
      "metadata": {
        "id": "YpTSr4ONbBIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset['input_ids'][201])"
      ],
      "metadata": {
        "id": "Li5GpijO6Jp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode_sentence=tokenizer.decode(train_dataset['input_ids'][201])\n",
        "print(decode_sentence)"
      ],
      "metadata": {
        "id": "W1bvtrnEt-jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a binary tensor indicating the position of the padded indices so that the model does not attend to them.\n",
        "print(train_dataset['attention_mask'][201])"
      ],
      "metadata": {
        "id": "355i-bcmr-ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning using TF Sequence Classifier\n",
        "# The contextual embeddings are input to the BERT for the downstream tasks\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import TFXLMRobertaForSequenceClassification\n",
        "#from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "#model=TFAutoModelForSequenceClassification.from_pretrained('jplu/tf-xlm-roberta-base',num_labels=2)\n",
        "model=TFXLMRobertaForSequenceClassification.from_pretrained('jplu/tf-xlm-roberta-base',num_labels=2)\n",
        "tf_train=train_dataset.remove_columns(['text']).with_format('tensorflow')\n",
        "tf_eval=eval_dataset.remove_columns(['text']).with_format('tensorflow')"
      ],
      "metadata": {
        "id": "fMh6GHGKURpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transforming the dataset into TF tensors - TF based classifier\n",
        "train_features={x:tf_train[x] for x in tokenizer.model_input_names}\n",
        "train_tf_dataset=tf.data.Dataset.from_tensor_slices((train_features,tf_train['labels']))\n",
        "train_tf_dataset=train_tf_dataset.shuffle(len(tf_train)).batch(8)\n",
        "\n",
        "eval_features={x:tf_eval[x] for x in tokenizer.model_input_names}\n",
        "eval_tf_dataset=tf.data.Dataset.from_tensor_slices((eval_features,tf_eval['labels']))\n",
        "eval_tf_dataset=eval_tf_dataset.shuffle(len(tf_eval)).batch(8)"
      ],
      "metadata": {
        "id": "BvhPlrXu4DFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model and fitting the dataset to the model\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.keras.metrics.SparseCategoricalAccuracy(),\n",
        ")    \n",
        "model.fit(train_tf_dataset,validation_data=eval_tf_dataset,epochs=2)\n"
      ],
      "metadata": {
        "id": "HmTk3NjU3yP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=load_dataset('csv',data_files={'test':'drive/MyDrive/test1.csv'})\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'],padding='max_length',truncation=True,max_length=100)\n",
        "tokenized_datasets=dataset.map(tokenize_function,batched=True)\n",
        "test_dataset=tokenized_datasets['test']\n",
        "tf_test=test_dataset.remove_columns(['text']).with_format('tensorflow')\n",
        "test_features={x:tf_test[x] for x in tokenizer.model_input_names}\n",
        "test_tf_dataset=tf.data.Dataset.from_tensor_slices((test_features,tf_test['label']))\n",
        "test_tf_dataset=test_tf_dataset.shuffle(len(tf_test)).batch(8)\n"
      ],
      "metadata": {
        "id": "vmAkvt_kHPWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(test_tf_dataset)['logits']\n",
        "class_preds = tf.math.argmax(preds,axis=1)"
      ],
      "metadata": {
        "id": "cMKJiqBM3znX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EVPP0xzhsrd"
      },
      "outputs": [],
      "source": [
        "print(class_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgkTkUDDf4Sk"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "metric = load_metric('accuracy')\n",
        "metric.compute(predictions=class_preds,references=tf_test['label'])"
      ]
    }
  ]
}